\documentclass{article}
\usepackage[margin=1in]{geometry}
\usepackage{qtree}
\usepackage{tikz}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{mathtools}
\usepackage{changepage}
\usepackage{setspace}
\usepackage{pifont}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\usepackage{color}
\usepackage{listings}
\geometry{a4paper}
\usetikzlibrary{arrows}
\lstset{ %
frame=single           % adds a frame around the code
}



\let\b\textbf                                               % BOLD TEXT
\let\i\textit                                               % ITALIC TEXT
\let\fnt\footnotesize
\let\s\scriptsize                                           % SMALL TEXT
\let\implies\Rightarrow                                     % IMPLIES SYMBOL
\let\t\text                                                 % TEXT IN MATH ENVIRONMENT
\let\l\left                                                 % BEGIN (,{,[,|,...
\let\r\right                                                % END ),},],|,...
\let\p\partial                                              % CURLY d SYMBOL
\let\del\nabla                                              % del OPERATOR
\let\bs\boldsymbol                                          % BOLD SYMBOLS
\let\ds\displaystyle                                        % \[ \] FORMATTING IN $ $
\let\mc\mathcal
\let\ig\includegraphics
\let\teq\triangleq
\let\mc\mathcal
\let\ms\mathscr
\let\mf\mathfrak
\let\mb\mathbb
\let\tt\texttt                                              % MONOSPACED TEXT
\let\mtt\mathtt


\newcommand{\aw}[1]{\begin{adjustwidth}{{#1}em}{{#1}em}}    % BEGIN adjustwidth {leftpad}{rightpad}
\newcommand{\eaw}{\end{adjustwidth}}                        % END adjustwidth
\newcommand{\bc}{\begin{center}}                            % BEGIN center
\newcommand{\ec}{\end{center}}                              % END center
\newcommand{\tab}{$\;\;\;\;$}                               % TAB HACK
\newcommand{\nl}{$\;$\\}                                    % NEW LINE HACK
\newcommand{\sfrac}[2]{{}^{#1}\!\! / \!{}_{#2}}                   % SMALL SLANTED FRACTION
\newcommand{\unit}[1]{\bs{\hat{{#1}}}}                      % UNIT VECTOR
\newcommand{\pdx}[3]{\frac{\p^{#1} {#2}}{\p {#3}^{#1}}}     % PARTIAL DERIVATIVE FRACTION
\newcommand{\tpdx}[3]{\tfrac{\p^{#1} {#2}}{\p {#3}^{#1}}}   % TINY \pdx
\newcommand{\ddx}[3]{\frac{d^{#1} {#2}}{d {#3}^{#1}}}       % DERIVATIVE FRACTION
\newcommand{\cross}{\! \times \!}                           % \! SMALLER SPACED CROSS
\newcommand{\dive}[1]{\del \! \cdot \! {#1}}                % DIVERGENCE
\newcommand{\curl}[1]{\del \cross {#1}}                     % CURL
\newcommand{\f}[2]{\frac{#1}{#2}}                           % FRAC SHORTCUT



\title{HW1 - High Performance Computing\\}
\author{Andrew Szymczak \\ February, 2015\\ }
\date{}
\begin{document}
\maketitle


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\aw{2}
\b{GitHub Repository}
\eaw
\aw{4}
\begin{itemize}

\end{itemize}


\eaw


\aw{2}
\b{MPI ring communication. Write a distributed memory program that sends an integer in a ring
starting from process 0 to 1 to 2 (and so on). The last process sends the message back to process
0 \\}
\eaw

\aw{4}
\begin{itemize}

    \item I allow two command line parameters $N$ and $M$. $N$ specifies the number of loops
        and $M$ specifies the number of integers sent in each message. Each message is an array
        of $M$ integers.
                \[ \texttt{msg =  (int *) malloc(sizeof(int) * M);} \]
        The first element is the only one altered as $\texttt{msg}$ is passed around;
        $\texttt{msg[0] += rank}$. If you don't input an argument or you input an illegal value,
        $N,M$ default to 1. On my machine $\texttt{ints}$ are 4 bytes, so to send 2MB messages I
        input $M=524288$. If the number of processors is less than 1, the program aborts.

    \item After the last message is received at processor zero, I verify that \\
        $\texttt{msg[0] = NP(P-1)/2}$ where $P$ is the number of processors.

    \item I only tested my program locally. The latency for a message seems to be about
        .026 ms. 20 processors were used to make the following plots. \\
                \bc
                \ig[scale = .25]{../Images/time} $\qquad$
                \ig[scale=.25]{../Images/latency}
                \ec

    \item With $P=20$, $N=10000$ and $M=524288$, a total of 200000 messages were sent in
        124.71 seconds. Since the limiting factor is bandwidth rather than latency, I can
        estimate a bandwidth of about 3.13 GB/s.

\end{itemize}
\eaw

\newpage

\aw{2}
\b{Distributed memory parallel Jacobi smoother. Use MPI to write a parallel version of the Jacobi smoother from Homework 0 \\}
\eaw

\aw{4}
\begin{itemize}
    \item The first and last element of $\bs{u}^{(i)}$ are the shared indices. First we pass
        the previous values $u^{(i)}_{-2} \rightarrow u^{(i+1)}_0$ for all $i<p\!-\!1$.
            Then we perform the Jacobi iteration. Then we pass the new values
            $u^{(i)}_1 \rightarrow u^{(i-1)}_{-1}$ for all $i>0$.
    \item I use an $\texttt{MPI\_Allreduce}$ to calculate the residual on every processor.
        Since this is an expensive process, I only do it once every 100 iterations.
    \item Since my machine only utilizes four processors, my first scaling test was for
        $p=1,2,3,4$. Let $I$ denote the number of iterations, and $t$ the total time in seconds.
                \[
                    \begin{array}{|c|c|c|c|c|}
                        \hline
                        N & I & p & \left| Au - f \right| & t \\ \hline
                        2520 & 5000000 & 1 & 0.932 & 54.62 \\
                        2520 & 5000000 & 2 & 0.932 & 28.26 \\
                        2520 & 5000000 & 3 & 0.932 & 22.08 \\
                        2520 & 5000000 & 4 & 0.932 & 20.12 \\ \hline
                    \end{array}
                \]
            \bc
            \ig[scale=.3]{../Images/jacobi1}
            \ec
        Note how the residual is independent of $p$, as expected. Each processor handles 2
        messages and $N/p\,$ of the Jacobi work in each iteration. Assuming full connectivity
        (so that all messages are done in parallel), we can expect $\mc{O}(\sfrac{N}{p})$ time
        (as marked by the $\mathtt{X}$'s). The slowdown is due to the suboptimal parallelisation
        and overhead of message passing.

    \item My second scaling test only establishes the high overhead of message passing.
        Increasing $p>4$ on my local machine effectively sequentializes crosstalk. Since there
        are $2p-2$ messages passed in every iteration, we can expect the computation time to
        grow $\sim p$.
                \[
                    \begin{array}{|c|c|c|c|c|}
                        \hline
                        N & I & p & \left| Au - f \right| & t \\ \hline
                        1200 & 100000 & 20 & 22.24 & 10.40 \\
                        1200 & 100000 & 40 & 22.24 & 21.83 \\
                        1200 & 100000 & 60 & 22.24 & 33.17 \\
                        1200 & 100000 & 80 & 22.24 & 48.06 \\
                        1200 & 100000 & 100 & 22.24 & 63.77 \\ \hline
                    \end{array}
                \]
    \item Gauss-Seidel would be more dificult to program because of the implicit dependence
        in each iteration. We could try to do something analagous to the first bullet point:
        First we pass the old values $u^{(i)}_{1} \rightarrow u^{(i-1)}_{-1}$ for all
        $i<p\!-\!1$. Then we perform the Jacobi iteration. Then we pass the new values
        $u^{(i)}_{-2} \rightarrow u^{(i+1)}_{0}$ for all $i<p\!-\!1$. The problem is that
        you can't do this in parallel. Before $u^{(i)}$ can preform Jacobi, it must wait
        to recieve the new value $u^{(i-1)}_{-2}$, which isn't sent until all lower rank
        processes are complete. The process is essentially sequential, and even worse off
        due to message passing.

\end{itemize}
\eaw

\end{document}


























